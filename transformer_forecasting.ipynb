{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Transformer for time-series forecasting  \n",
    "\n",
    "The following code is deomo to apply Transformer encoder module to predict daily temperature. The `PyTorch` provides build-in `TransformerEncoderLayer` and `TransformerDncoderLayer`, so we needn't write it by ourselves. If you are interested in how to build an encoder and decoder from scratch. Please kindly read [pytorch_from_scratch.ipynb](./pytorch_from_scratch.ipynb)\n",
    "\n",
    "Reference:  \n",
    "[1] https://github.com/oliverguhr/transformer-time-series-prediction/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "#\n",
    "#print(out)\n",
    "\n",
    "input_window = 100\n",
    "output_window = 5\n",
    "batch_size = 10 # batch size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "       \n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=250,num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
    "        train_label = input_data[i:i+tw]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "def get_data():\n",
    "    t        = np.arange(0, 400, 0.1)\n",
    "    amplitude   = np.sin(t) + np.sin(t*0.05) +np.sin(t*0.12) *np.random.normal(-0.2, 0.2, len(t))\n",
    "    \n",
    "    # from pandas import read_csv\n",
    "    # series = read_csv('./data/daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    # amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1) # (4000,)\n",
    "    \n",
    "    \n",
    "    sampels = 2800\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment.. \n",
    "    train_sequence = create_inout_sequences(train_data,input_window)\n",
    "    train_sequence = train_sequence[:-output_window] #todo: fix hack?\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1) \n",
    "    test_data = create_inout_sequences(test_data,input_window)\n",
    "    test_data = test_data[:-output_window] #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device)\n",
    "\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]    \n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        # data [100, 10, 1]\n",
    "        # targets [100, 10, 1]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) # [100, 10, 1]\n",
    "\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    if not os.path.exists(\"./graph\"):\n",
    "        os.makedirs(\"./graph\")\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth[:500],color=\"blue\")\n",
    "    pyplot.plot(test_result-truth,color=\"green\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    pyplot.close()\n",
    "    \n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%steps)\n",
    "    pyplot.close()\n",
    "        \n",
    "# entweder ist hier ein fehler im loss oder in der train methode, aber die ergebnisse sind unterschiedlich \n",
    "# auch zu denen der predict_future\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).cpu().item()            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2947d93edf5f>:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  return torch.FloatTensor(inout_seq)\n",
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    53/  269 batches | lr 0.005000 | 29.48 ms | loss 5.53086 | ppl   252.36\n",
      "| epoch   1 |   106/  269 batches | lr 0.005000 |  3.40 ms | loss 0.17483 | ppl     1.19\n",
      "| epoch   1 |   159/  269 batches | lr 0.005000 |  3.45 ms | loss 0.13412 | ppl     1.14\n",
      "| epoch   1 |   212/  269 batches | lr 0.005000 |  3.40 ms | loss 0.21703 | ppl     1.24\n",
      "| epoch   1 |   265/  269 batches | lr 0.005000 |  3.36 ms | loss 0.09104 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.40s | valid loss 0.74906 | valid ppl     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    53/  269 batches | lr 0.004802 |  3.49 ms | loss 0.11046 | ppl     1.12\n",
      "| epoch   2 |   106/  269 batches | lr 0.004802 |  3.44 ms | loss 0.06811 | ppl     1.07\n",
      "| epoch   2 |   159/  269 batches | lr 0.004802 |  3.42 ms | loss 0.10746 | ppl     1.11\n",
      "| epoch   2 |   212/  269 batches | lr 0.004802 |  3.44 ms | loss 0.04397 | ppl     1.04\n",
      "| epoch   2 |   265/  269 batches | lr 0.004802 |  3.34 ms | loss 0.03269 | ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.96s | valid loss 0.54565 | valid ppl     1.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    53/  269 batches | lr 0.004706 |  3.49 ms | loss 0.07649 | ppl     1.08\n",
      "| epoch   3 |   106/  269 batches | lr 0.004706 |  3.34 ms | loss 0.03195 | ppl     1.03\n",
      "| epoch   3 |   159/  269 batches | lr 0.004706 |  3.38 ms | loss 0.02550 | ppl     1.03\n",
      "| epoch   3 |   212/  269 batches | lr 0.004706 |  3.40 ms | loss 0.01764 | ppl     1.02\n",
      "| epoch   3 |   265/  269 batches | lr 0.004706 |  3.39 ms | loss 0.01308 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.95s | valid loss 0.38115 | valid ppl     1.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    53/  269 batches | lr 0.004612 |  3.57 ms | loss 0.03374 | ppl     1.03\n",
      "| epoch   4 |   106/  269 batches | lr 0.004612 |  3.49 ms | loss 0.01465 | ppl     1.01\n",
      "| epoch   4 |   159/  269 batches | lr 0.004612 |  3.41 ms | loss 0.01533 | ppl     1.02\n",
      "| epoch   4 |   212/  269 batches | lr 0.004612 |  3.55 ms | loss 0.01199 | ppl     1.01\n",
      "| epoch   4 |   265/  269 batches | lr 0.004612 |  3.38 ms | loss 0.01116 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.97s | valid loss 0.43524 | valid ppl     1.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    53/  269 batches | lr 0.004520 |  3.44 ms | loss 0.02932 | ppl     1.03\n",
      "| epoch   5 |   106/  269 batches | lr 0.004520 |  3.51 ms | loss 0.01424 | ppl     1.01\n",
      "| epoch   5 |   159/  269 batches | lr 0.004520 |  3.36 ms | loss 0.01184 | ppl     1.01\n",
      "| epoch   5 |   212/  269 batches | lr 0.004520 |  3.40 ms | loss 0.01294 | ppl     1.01\n",
      "| epoch   5 |   265/  269 batches | lr 0.004520 |  3.44 ms | loss 0.01187 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.96s | valid loss 0.39199 | valid ppl     1.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    53/  269 batches | lr 0.004429 |  3.47 ms | loss 0.03646 | ppl     1.04\n",
      "| epoch   6 |   106/  269 batches | lr 0.004429 |  3.46 ms | loss 0.01547 | ppl     1.02\n",
      "| epoch   6 |   159/  269 batches | lr 0.004429 |  3.36 ms | loss 0.01096 | ppl     1.01\n",
      "| epoch   6 |   212/  269 batches | lr 0.004429 |  3.38 ms | loss 0.01456 | ppl     1.01\n",
      "| epoch   6 |   265/  269 batches | lr 0.004429 |  3.40 ms | loss 0.01660 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.95s | valid loss 0.38630 | valid ppl     1.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    53/  269 batches | lr 0.004341 |  3.51 ms | loss 0.02444 | ppl     1.02\n",
      "| epoch   7 |   106/  269 batches | lr 0.004341 |  3.46 ms | loss 0.01469 | ppl     1.01\n",
      "| epoch   7 |   159/  269 batches | lr 0.004341 |  3.42 ms | loss 0.01212 | ppl     1.01\n",
      "| epoch   7 |   212/  269 batches | lr 0.004341 |  3.38 ms | loss 0.01281 | ppl     1.01\n",
      "| epoch   7 |   265/  269 batches | lr 0.004341 |  3.44 ms | loss 0.01107 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.96s | valid loss 0.36647 | valid ppl     1.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    53/  269 batches | lr 0.004254 |  3.47 ms | loss 0.03030 | ppl     1.03\n",
      "| epoch   8 |   106/  269 batches | lr 0.004254 |  3.42 ms | loss 0.01680 | ppl     1.02\n",
      "| epoch   8 |   159/  269 batches | lr 0.004254 |  3.44 ms | loss 0.01200 | ppl     1.01\n",
      "| epoch   8 |   212/  269 batches | lr 0.004254 |  3.38 ms | loss 0.01517 | ppl     1.02\n",
      "| epoch   8 |   265/  269 batches | lr 0.004254 |  3.38 ms | loss 0.01419 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.95s | valid loss 0.40951 | valid ppl     1.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    53/  269 batches | lr 0.004169 |  3.51 ms | loss 0.03239 | ppl     1.03\n",
      "| epoch   9 |   106/  269 batches | lr 0.004169 |  3.38 ms | loss 0.01524 | ppl     1.02\n",
      "| epoch   9 |   159/  269 batches | lr 0.004169 |  3.38 ms | loss 0.01342 | ppl     1.01\n",
      "| epoch   9 |   212/  269 batches | lr 0.004169 |  3.34 ms | loss 0.01179 | ppl     1.01\n",
      "| epoch   9 |   265/  269 batches | lr 0.004169 |  3.34 ms | loss 0.01206 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.95s | valid loss 0.35918 | valid ppl     1.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    53/  269 batches | lr 0.004085 |  3.46 ms | loss 0.03047 | ppl     1.03\n",
      "| epoch  10 |   106/  269 batches | lr 0.004085 |  3.47 ms | loss 0.01415 | ppl     1.01\n",
      "| epoch  10 |   159/  269 batches | lr 0.004085 |  3.38 ms | loss 0.01085 | ppl     1.01\n",
      "| epoch  10 |   212/  269 batches | lr 0.004085 |  3.51 ms | loss 0.01090 | ppl     1.01\n",
      "| epoch  10 |   265/  269 batches | lr 0.004085 |  3.40 ms | loss 0.01304 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.31s | valid loss 0.34709 | valid ppl     1.41\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  11 |    53/  269 batches | lr 0.004004 |  3.49 ms | loss 0.02862 | ppl     1.03\n",
      "| epoch  11 |   106/  269 batches | lr 0.004004 |  3.42 ms | loss 0.01439 | ppl     1.01\n",
      "| epoch  11 |   159/  269 batches | lr 0.004004 |  3.51 ms | loss 0.01274 | ppl     1.01\n",
      "| epoch  11 |   212/  269 batches | lr 0.004004 |  3.49 ms | loss 0.01204 | ppl     1.01\n",
      "| epoch  11 |   265/  269 batches | lr 0.004004 |  3.46 ms | loss 0.01389 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.97s | valid loss 0.34629 | valid ppl     1.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    53/  269 batches | lr 0.003924 |  3.64 ms | loss 0.03075 | ppl     1.03\n",
      "| epoch  12 |   106/  269 batches | lr 0.003924 |  3.51 ms | loss 0.01323 | ppl     1.01\n",
      "| epoch  12 |   159/  269 batches | lr 0.003924 |  3.51 ms | loss 0.01275 | ppl     1.01\n",
      "| epoch  12 |   212/  269 batches | lr 0.003924 |  3.51 ms | loss 0.01119 | ppl     1.01\n",
      "| epoch  12 |   265/  269 batches | lr 0.003924 |  3.46 ms | loss 0.00927 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.98s | valid loss 0.31766 | valid ppl     1.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    53/  269 batches | lr 0.003845 |  3.51 ms | loss 0.02604 | ppl     1.03\n",
      "| epoch  13 |   106/  269 batches | lr 0.003845 |  3.47 ms | loss 0.01069 | ppl     1.01\n",
      "| epoch  13 |   159/  269 batches | lr 0.003845 |  3.46 ms | loss 0.01222 | ppl     1.01\n",
      "| epoch  13 |   212/  269 batches | lr 0.003845 |  3.47 ms | loss 0.01128 | ppl     1.01\n",
      "| epoch  13 |   265/  269 batches | lr 0.003845 |  3.42 ms | loss 0.01043 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.97s | valid loss 0.29131 | valid ppl     1.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    53/  269 batches | lr 0.003768 |  3.44 ms | loss 0.02613 | ppl     1.03\n",
      "| epoch  14 |   106/  269 batches | lr 0.003768 |  3.40 ms | loss 0.01022 | ppl     1.01\n",
      "| epoch  14 |   159/  269 batches | lr 0.003768 |  3.41 ms | loss 0.01246 | ppl     1.01\n",
      "| epoch  14 |   212/  269 batches | lr 0.003768 |  3.38 ms | loss 0.01121 | ppl     1.01\n",
      "| epoch  14 |   265/  269 batches | lr 0.003768 |  3.40 ms | loss 0.00999 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.95s | valid loss 0.28867 | valid ppl     1.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    53/  269 batches | lr 0.003693 |  3.53 ms | loss 0.03984 | ppl     1.04\n",
      "| epoch  15 |   106/  269 batches | lr 0.003693 |  3.42 ms | loss 0.01535 | ppl     1.02\n",
      "| epoch  15 |   159/  269 batches | lr 0.003693 |  3.40 ms | loss 0.01174 | ppl     1.01\n",
      "| epoch  15 |   212/  269 batches | lr 0.003693 |  3.34 ms | loss 0.01135 | ppl     1.01\n",
      "| epoch  15 |   265/  269 batches | lr 0.003693 |  3.38 ms | loss 0.00999 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.95s | valid loss 0.27375 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    53/  269 batches | lr 0.003619 |  3.51 ms | loss 0.03512 | ppl     1.04\n",
      "| epoch  16 |   106/  269 batches | lr 0.003619 |  3.40 ms | loss 0.01490 | ppl     1.02\n",
      "| epoch  16 |   159/  269 batches | lr 0.003619 |  3.42 ms | loss 0.01103 | ppl     1.01\n",
      "| epoch  16 |   212/  269 batches | lr 0.003619 |  3.40 ms | loss 0.01303 | ppl     1.01\n",
      "| epoch  16 |   265/  269 batches | lr 0.003619 |  3.40 ms | loss 0.01080 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.96s | valid loss 0.31552 | valid ppl     1.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    53/  269 batches | lr 0.003547 |  3.47 ms | loss 0.03583 | ppl     1.04\n",
      "| epoch  17 |   106/  269 batches | lr 0.003547 |  3.38 ms | loss 0.01060 | ppl     1.01\n",
      "| epoch  17 |   159/  269 batches | lr 0.003547 |  3.40 ms | loss 0.01119 | ppl     1.01\n",
      "| epoch  17 |   212/  269 batches | lr 0.003547 |  3.46 ms | loss 0.01310 | ppl     1.01\n",
      "| epoch  17 |   265/  269 batches | lr 0.003547 |  3.34 ms | loss 0.01111 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.95s | valid loss 0.29348 | valid ppl     1.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    53/  269 batches | lr 0.003476 |  3.46 ms | loss 0.03507 | ppl     1.04\n",
      "| epoch  18 |   106/  269 batches | lr 0.003476 |  3.50 ms | loss 0.01449 | ppl     1.01\n",
      "| epoch  18 |   159/  269 batches | lr 0.003476 |  3.40 ms | loss 0.01127 | ppl     1.01\n",
      "| epoch  18 |   212/  269 batches | lr 0.003476 |  3.57 ms | loss 0.01453 | ppl     1.01\n",
      "| epoch  18 |   265/  269 batches | lr 0.003476 |  3.90 ms | loss 0.01079 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  1.00s | valid loss 0.31356 | valid ppl     1.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    53/  269 batches | lr 0.003406 |  3.74 ms | loss 0.03282 | ppl     1.03\n",
      "| epoch  19 |   106/  269 batches | lr 0.003406 |  3.57 ms | loss 0.01366 | ppl     1.01\n",
      "| epoch  19 |   159/  269 batches | lr 0.003406 |  3.57 ms | loss 0.01342 | ppl     1.01\n",
      "| epoch  19 |   212/  269 batches | lr 0.003406 |  3.73 ms | loss 0.01237 | ppl     1.01\n",
      "| epoch  19 |   265/  269 batches | lr 0.003406 |  4.30 ms | loss 0.01521 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  1.06s | valid loss 0.30962 | valid ppl     1.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    53/  269 batches | lr 0.003338 |  4.29 ms | loss 0.03709 | ppl     1.04\n",
      "| epoch  20 |   106/  269 batches | lr 0.003338 |  3.64 ms | loss 0.01395 | ppl     1.01\n",
      "| epoch  20 |   159/  269 batches | lr 0.003338 |  4.08 ms | loss 0.01091 | ppl     1.01\n",
      "| epoch  20 |   212/  269 batches | lr 0.003338 |  4.41 ms | loss 0.01214 | ppl     1.01\n",
      "| epoch  20 |   265/  269 batches | lr 0.003338 |  4.25 ms | loss 0.00997 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  2.43s | valid loss 0.31364 | valid ppl     1.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    53/  269 batches | lr 0.003271 |  3.55 ms | loss 0.04110 | ppl     1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  21 |   106/  269 batches | lr 0.003271 |  3.46 ms | loss 0.01771 | ppl     1.02\n",
      "| epoch  21 |   159/  269 batches | lr 0.003271 |  3.57 ms | loss 0.01150 | ppl     1.01\n",
      "| epoch  21 |   212/  269 batches | lr 0.003271 |  3.62 ms | loss 0.01368 | ppl     1.01\n",
      "| epoch  21 |   265/  269 batches | lr 0.003271 |  3.58 ms | loss 0.01175 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  1.00s | valid loss 0.28336 | valid ppl     1.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    53/  269 batches | lr 0.003206 |  3.66 ms | loss 0.03514 | ppl     1.04\n",
      "| epoch  22 |   106/  269 batches | lr 0.003206 |  3.63 ms | loss 0.01420 | ppl     1.01\n",
      "| epoch  22 |   159/  269 batches | lr 0.003206 |  3.50 ms | loss 0.01011 | ppl     1.01\n",
      "| epoch  22 |   212/  269 batches | lr 0.003206 |  3.49 ms | loss 0.01341 | ppl     1.01\n",
      "| epoch  22 |   265/  269 batches | lr 0.003206 |  3.53 ms | loss 0.00982 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  1.00s | valid loss 0.29117 | valid ppl     1.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    53/  269 batches | lr 0.003142 |  3.55 ms | loss 0.03968 | ppl     1.04\n",
      "| epoch  23 |   106/  269 batches | lr 0.003142 |  3.51 ms | loss 0.01533 | ppl     1.02\n",
      "| epoch  23 |   159/  269 batches | lr 0.003142 |  3.46 ms | loss 0.00952 | ppl     1.01\n",
      "| epoch  23 |   212/  269 batches | lr 0.003142 |  3.51 ms | loss 0.01313 | ppl     1.01\n",
      "| epoch  23 |   265/  269 batches | lr 0.003142 |  3.40 ms | loss 0.01068 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  0.97s | valid loss 0.29652 | valid ppl     1.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    53/  269 batches | lr 0.003079 |  3.61 ms | loss 0.04383 | ppl     1.04\n",
      "| epoch  24 |   106/  269 batches | lr 0.003079 |  3.51 ms | loss 0.02648 | ppl     1.03\n",
      "| epoch  24 |   159/  269 batches | lr 0.003079 |  3.47 ms | loss 0.01037 | ppl     1.01\n",
      "| epoch  24 |   212/  269 batches | lr 0.003079 |  3.63 ms | loss 0.01516 | ppl     1.02\n",
      "| epoch  24 |   265/  269 batches | lr 0.003079 |  3.65 ms | loss 0.01081 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  1.00s | valid loss 0.28128 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    53/  269 batches | lr 0.003017 |  3.76 ms | loss 0.03479 | ppl     1.04\n",
      "| epoch  25 |   106/  269 batches | lr 0.003017 |  3.65 ms | loss 0.01968 | ppl     1.02\n",
      "| epoch  25 |   159/  269 batches | lr 0.003017 |  3.52 ms | loss 0.01125 | ppl     1.01\n",
      "| epoch  25 |   212/  269 batches | lr 0.003017 |  3.57 ms | loss 0.01285 | ppl     1.01\n",
      "| epoch  25 |   265/  269 batches | lr 0.003017 |  3.53 ms | loss 0.01175 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  1.01s | valid loss 0.26966 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    53/  269 batches | lr 0.002957 |  3.69 ms | loss 0.03363 | ppl     1.03\n",
      "| epoch  26 |   106/  269 batches | lr 0.002957 |  3.51 ms | loss 0.02354 | ppl     1.02\n",
      "| epoch  26 |   159/  269 batches | lr 0.002957 |  3.53 ms | loss 0.01163 | ppl     1.01\n",
      "| epoch  26 |   212/  269 batches | lr 0.002957 |  3.54 ms | loss 0.01205 | ppl     1.01\n",
      "| epoch  26 |   265/  269 batches | lr 0.002957 |  3.53 ms | loss 0.01125 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  0.99s | valid loss 0.30544 | valid ppl     1.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    53/  269 batches | lr 0.002898 |  3.57 ms | loss 0.03522 | ppl     1.04\n",
      "| epoch  27 |   106/  269 batches | lr 0.002898 |  3.50 ms | loss 0.02693 | ppl     1.03\n",
      "| epoch  27 |   159/  269 batches | lr 0.002898 |  3.58 ms | loss 0.01069 | ppl     1.01\n",
      "| epoch  27 |   212/  269 batches | lr 0.002898 |  3.61 ms | loss 0.01162 | ppl     1.01\n",
      "| epoch  27 |   265/  269 batches | lr 0.002898 |  3.70 ms | loss 0.01016 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  1.00s | valid loss 0.27496 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    53/  269 batches | lr 0.002840 |  3.87 ms | loss 0.03125 | ppl     1.03\n",
      "| epoch  28 |   106/  269 batches | lr 0.002840 |  3.76 ms | loss 0.01727 | ppl     1.02\n",
      "| epoch  28 |   159/  269 batches | lr 0.002840 |  3.72 ms | loss 0.01012 | ppl     1.01\n",
      "| epoch  28 |   212/  269 batches | lr 0.002840 |  3.61 ms | loss 0.01214 | ppl     1.01\n",
      "| epoch  28 |   265/  269 batches | lr 0.002840 |  3.65 ms | loss 0.00936 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  1.04s | valid loss 0.28606 | valid ppl     1.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    53/  269 batches | lr 0.002783 |  3.76 ms | loss 0.02789 | ppl     1.03\n",
      "| epoch  29 |   106/  269 batches | lr 0.002783 |  3.72 ms | loss 0.01965 | ppl     1.02\n",
      "| epoch  29 |   159/  269 batches | lr 0.002783 |  3.63 ms | loss 0.01011 | ppl     1.01\n",
      "| epoch  29 |   212/  269 batches | lr 0.002783 |  3.82 ms | loss 0.01189 | ppl     1.01\n",
      "| epoch  29 |   265/  269 batches | lr 0.002783 |  3.69 ms | loss 0.00986 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  1.04s | valid loss 0.26375 | valid ppl     1.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    53/  269 batches | lr 0.002727 |  3.69 ms | loss 0.03017 | ppl     1.03\n",
      "| epoch  30 |   106/  269 batches | lr 0.002727 |  3.53 ms | loss 0.02114 | ppl     1.02\n",
      "| epoch  30 |   159/  269 batches | lr 0.002727 |  3.61 ms | loss 0.01036 | ppl     1.01\n",
      "| epoch  30 |   212/  269 batches | lr 0.002727 |  3.75 ms | loss 0.01067 | ppl     1.01\n",
      "| epoch  30 |   265/  269 batches | lr 0.002727 |  3.62 ms | loss 0.01268 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  2.28s | valid loss 0.29785 | valid ppl     1.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    53/  269 batches | lr 0.002673 |  3.59 ms | loss 0.03053 | ppl     1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  31 |   106/  269 batches | lr 0.002673 |  3.54 ms | loss 0.01961 | ppl     1.02\n",
      "| epoch  31 |   159/  269 batches | lr 0.002673 |  3.51 ms | loss 0.01176 | ppl     1.01\n",
      "| epoch  31 |   212/  269 batches | lr 0.002673 |  3.55 ms | loss 0.01362 | ppl     1.01\n",
      "| epoch  31 |   265/  269 batches | lr 0.002673 |  3.49 ms | loss 0.01128 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  0.99s | valid loss 0.36737 | valid ppl     1.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    53/  269 batches | lr 0.002619 |  3.63 ms | loss 0.02727 | ppl     1.03\n",
      "| epoch  32 |   106/  269 batches | lr 0.002619 |  3.56 ms | loss 0.01893 | ppl     1.02\n",
      "| epoch  32 |   159/  269 batches | lr 0.002619 |  3.68 ms | loss 0.01255 | ppl     1.01\n",
      "| epoch  32 |   212/  269 batches | lr 0.002619 |  3.64 ms | loss 0.01098 | ppl     1.01\n",
      "| epoch  32 |   265/  269 batches | lr 0.002619 |  3.59 ms | loss 0.00895 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  1.01s | valid loss 0.33545 | valid ppl     1.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    53/  269 batches | lr 0.002567 |  3.71 ms | loss 0.02806 | ppl     1.03\n",
      "| epoch  33 |   106/  269 batches | lr 0.002567 |  3.53 ms | loss 0.01439 | ppl     1.01\n",
      "| epoch  33 |   159/  269 batches | lr 0.002567 |  3.55 ms | loss 0.01145 | ppl     1.01\n",
      "| epoch  33 |   212/  269 batches | lr 0.002567 |  3.55 ms | loss 0.01149 | ppl     1.01\n",
      "| epoch  33 |   265/  269 batches | lr 0.002567 |  3.49 ms | loss 0.00934 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  1.00s | valid loss 0.30524 | valid ppl     1.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    53/  269 batches | lr 0.002516 |  3.55 ms | loss 0.02571 | ppl     1.03\n",
      "| epoch  34 |   106/  269 batches | lr 0.002516 |  3.57 ms | loss 0.01567 | ppl     1.02\n",
      "| epoch  34 |   159/  269 batches | lr 0.002516 |  3.59 ms | loss 0.01275 | ppl     1.01\n",
      "| epoch  34 |   212/  269 batches | lr 0.002516 |  4.02 ms | loss 0.00924 | ppl     1.01\n",
      "| epoch  34 |   265/  269 batches | lr 0.002516 |  3.94 ms | loss 0.00873 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  1.05s | valid loss 0.34348 | valid ppl     1.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    53/  269 batches | lr 0.002465 |  4.25 ms | loss 0.02378 | ppl     1.02\n",
      "| epoch  35 |   106/  269 batches | lr 0.002465 |  3.66 ms | loss 0.01213 | ppl     1.01\n",
      "| epoch  35 |   159/  269 batches | lr 0.002465 |  4.37 ms | loss 0.01137 | ppl     1.01\n",
      "| epoch  35 |   212/  269 batches | lr 0.002465 |  3.88 ms | loss 0.00989 | ppl     1.01\n",
      "| epoch  35 |   265/  269 batches | lr 0.002465 |  3.49 ms | loss 0.00809 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  1.09s | valid loss 0.34534 | valid ppl     1.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    53/  269 batches | lr 0.002416 |  3.46 ms | loss 0.02659 | ppl     1.03\n",
      "| epoch  36 |   106/  269 batches | lr 0.002416 |  3.47 ms | loss 0.01166 | ppl     1.01\n",
      "| epoch  36 |   159/  269 batches | lr 0.002416 |  3.47 ms | loss 0.01100 | ppl     1.01\n",
      "| epoch  36 |   212/  269 batches | lr 0.002416 |  3.40 ms | loss 0.01043 | ppl     1.01\n",
      "| epoch  36 |   265/  269 batches | lr 0.002416 |  3.44 ms | loss 0.00825 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  0.96s | valid loss 0.36699 | valid ppl     1.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    53/  269 batches | lr 0.002368 |  3.46 ms | loss 0.02803 | ppl     1.03\n",
      "| epoch  37 |   106/  269 batches | lr 0.002368 |  3.46 ms | loss 0.01072 | ppl     1.01\n",
      "| epoch  37 |   159/  269 batches | lr 0.002368 |  3.44 ms | loss 0.01221 | ppl     1.01\n",
      "| epoch  37 |   212/  269 batches | lr 0.002368 |  3.46 ms | loss 0.00963 | ppl     1.01\n",
      "| epoch  37 |   265/  269 batches | lr 0.002368 |  3.47 ms | loss 0.00771 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  0.96s | valid loss 0.38469 | valid ppl     1.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    53/  269 batches | lr 0.002320 |  3.49 ms | loss 0.02685 | ppl     1.03\n",
      "| epoch  38 |   106/  269 batches | lr 0.002320 |  3.44 ms | loss 0.01054 | ppl     1.01\n",
      "| epoch  38 |   159/  269 batches | lr 0.002320 |  3.49 ms | loss 0.01254 | ppl     1.01\n",
      "| epoch  38 |   212/  269 batches | lr 0.002320 |  3.47 ms | loss 0.00893 | ppl     1.01\n",
      "| epoch  38 |   265/  269 batches | lr 0.002320 |  3.44 ms | loss 0.00836 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  0.97s | valid loss 0.39486 | valid ppl     1.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    53/  269 batches | lr 0.002274 |  3.51 ms | loss 0.02767 | ppl     1.03\n",
      "| epoch  39 |   106/  269 batches | lr 0.002274 |  3.38 ms | loss 0.01007 | ppl     1.01\n",
      "| epoch  39 |   159/  269 batches | lr 0.002274 |  3.40 ms | loss 0.01262 | ppl     1.01\n",
      "| epoch  39 |   212/  269 batches | lr 0.002274 |  3.44 ms | loss 0.00894 | ppl     1.01\n",
      "| epoch  39 |   265/  269 batches | lr 0.002274 |  3.42 ms | loss 0.00773 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  0.96s | valid loss 0.39815 | valid ppl     1.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    53/  269 batches | lr 0.002229 |  3.49 ms | loss 0.02734 | ppl     1.03\n",
      "| epoch  40 |   106/  269 batches | lr 0.002229 |  3.47 ms | loss 0.01055 | ppl     1.01\n",
      "| epoch  40 |   159/  269 batches | lr 0.002229 |  3.46 ms | loss 0.01364 | ppl     1.01\n",
      "| epoch  40 |   212/  269 batches | lr 0.002229 |  3.40 ms | loss 0.00919 | ppl     1.01\n",
      "| epoch  40 |   265/  269 batches | lr 0.002229 |  3.46 ms | loss 0.00769 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  2.23s | valid loss 0.39941 | valid ppl     1.49\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  41 |    53/  269 batches | lr 0.002184 |  3.53 ms | loss 0.03041 | ppl     1.03\n",
      "| epoch  41 |   106/  269 batches | lr 0.002184 |  3.46 ms | loss 0.01047 | ppl     1.01\n",
      "| epoch  41 |   159/  269 batches | lr 0.002184 |  3.52 ms | loss 0.01478 | ppl     1.01\n",
      "| epoch  41 |   212/  269 batches | lr 0.002184 |  3.44 ms | loss 0.00955 | ppl     1.01\n",
      "| epoch  41 |   265/  269 batches | lr 0.002184 |  3.44 ms | loss 0.00786 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  0.97s | valid loss 0.38118 | valid ppl     1.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    53/  269 batches | lr 0.002140 |  3.53 ms | loss 0.03481 | ppl     1.04\n",
      "| epoch  42 |   106/  269 batches | lr 0.002140 |  3.44 ms | loss 0.01406 | ppl     1.01\n",
      "| epoch  42 |   159/  269 batches | lr 0.002140 |  3.47 ms | loss 0.01904 | ppl     1.02\n",
      "| epoch  42 |   212/  269 batches | lr 0.002140 |  3.53 ms | loss 0.00919 | ppl     1.01\n",
      "| epoch  42 |   265/  269 batches | lr 0.002140 |  3.44 ms | loss 0.00868 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  0.97s | valid loss 0.37913 | valid ppl     1.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    53/  269 batches | lr 0.002097 |  3.47 ms | loss 0.04149 | ppl     1.04\n",
      "| epoch  43 |   106/  269 batches | lr 0.002097 |  3.38 ms | loss 0.02708 | ppl     1.03\n",
      "| epoch  43 |   159/  269 batches | lr 0.002097 |  3.44 ms | loss 0.02495 | ppl     1.03\n",
      "| epoch  43 |   212/  269 batches | lr 0.002097 |  3.43 ms | loss 0.01077 | ppl     1.01\n",
      "| epoch  43 |   265/  269 batches | lr 0.002097 |  3.40 ms | loss 0.00915 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  0.96s | valid loss 0.34460 | valid ppl     1.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    53/  269 batches | lr 0.002055 |  3.53 ms | loss 0.04142 | ppl     1.04\n",
      "| epoch  44 |   106/  269 batches | lr 0.002055 |  3.46 ms | loss 0.03177 | ppl     1.03\n",
      "| epoch  44 |   159/  269 batches | lr 0.002055 |  3.47 ms | loss 0.02209 | ppl     1.02\n",
      "| epoch  44 |   212/  269 batches | lr 0.002055 |  3.49 ms | loss 0.01057 | ppl     1.01\n",
      "| epoch  44 |   265/  269 batches | lr 0.002055 |  3.46 ms | loss 0.00953 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  0.97s | valid loss 0.32088 | valid ppl     1.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    53/  269 batches | lr 0.002014 |  3.50 ms | loss 0.04478 | ppl     1.05\n",
      "| epoch  45 |   106/  269 batches | lr 0.002014 |  3.44 ms | loss 0.04687 | ppl     1.05\n",
      "| epoch  45 |   159/  269 batches | lr 0.002014 |  3.38 ms | loss 0.02400 | ppl     1.02\n",
      "| epoch  45 |   212/  269 batches | lr 0.002014 |  3.40 ms | loss 0.01136 | ppl     1.01\n",
      "| epoch  45 |   265/  269 batches | lr 0.002014 |  3.57 ms | loss 0.00987 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  0.97s | valid loss 0.28329 | valid ppl     1.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    53/  269 batches | lr 0.001974 |  3.53 ms | loss 0.04297 | ppl     1.04\n",
      "| epoch  46 |   106/  269 batches | lr 0.001974 |  3.48 ms | loss 0.05577 | ppl     1.06\n",
      "| epoch  46 |   159/  269 batches | lr 0.001974 |  3.42 ms | loss 0.02213 | ppl     1.02\n",
      "| epoch  46 |   212/  269 batches | lr 0.001974 |  3.49 ms | loss 0.01143 | ppl     1.01\n",
      "| epoch  46 |   265/  269 batches | lr 0.001974 |  3.44 ms | loss 0.01002 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  0.97s | valid loss 0.25485 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    53/  269 batches | lr 0.001935 |  3.51 ms | loss 0.03735 | ppl     1.04\n",
      "| epoch  47 |   106/  269 batches | lr 0.001935 |  3.57 ms | loss 0.05290 | ppl     1.05\n",
      "| epoch  47 |   159/  269 batches | lr 0.001935 |  3.42 ms | loss 0.02768 | ppl     1.03\n",
      "| epoch  47 |   212/  269 batches | lr 0.001935 |  3.44 ms | loss 0.01304 | ppl     1.01\n",
      "| epoch  47 |   265/  269 batches | lr 0.001935 |  3.36 ms | loss 0.01044 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  0.97s | valid loss 0.25194 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    53/  269 batches | lr 0.001896 |  3.51 ms | loss 0.03668 | ppl     1.04\n",
      "| epoch  48 |   106/  269 batches | lr 0.001896 |  3.42 ms | loss 0.05885 | ppl     1.06\n",
      "| epoch  48 |   159/  269 batches | lr 0.001896 |  3.40 ms | loss 0.02883 | ppl     1.03\n",
      "| epoch  48 |   212/  269 batches | lr 0.001896 |  3.44 ms | loss 0.01410 | ppl     1.01\n",
      "| epoch  48 |   265/  269 batches | lr 0.001896 |  3.46 ms | loss 0.01149 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  0.96s | valid loss 0.24695 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |    53/  269 batches | lr 0.001858 |  3.50 ms | loss 0.03616 | ppl     1.04\n",
      "| epoch  49 |   106/  269 batches | lr 0.001858 |  3.40 ms | loss 0.05411 | ppl     1.06\n",
      "| epoch  49 |   159/  269 batches | lr 0.001858 |  3.42 ms | loss 0.03588 | ppl     1.04\n",
      "| epoch  49 |   212/  269 batches | lr 0.001858 |  3.44 ms | loss 0.01499 | ppl     1.02\n",
      "| epoch  49 |   265/  269 batches | lr 0.001858 |  3.46 ms | loss 0.01400 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  0.96s | valid loss 0.22662 | valid ppl     1.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    53/  269 batches | lr 0.001821 |  3.44 ms | loss 0.03641 | ppl     1.04\n",
      "| epoch  50 |   106/  269 batches | lr 0.001821 |  3.51 ms | loss 0.04691 | ppl     1.05\n",
      "| epoch  50 |   159/  269 batches | lr 0.001821 |  3.47 ms | loss 0.03087 | ppl     1.03\n",
      "| epoch  50 |   212/  269 batches | lr 0.001821 |  3.40 ms | loss 0.01412 | ppl     1.01\n",
      "| epoch  50 |   265/  269 batches | lr 0.001821 |  3.46 ms | loss 0.01331 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  2.21s | valid loss 0.20094 | valid ppl     1.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |    53/  269 batches | lr 0.001784 |  3.47 ms | loss 0.03539 | ppl     1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  51 |   106/  269 batches | lr 0.001784 |  3.44 ms | loss 0.05072 | ppl     1.05\n",
      "| epoch  51 |   159/  269 batches | lr 0.001784 |  3.46 ms | loss 0.03406 | ppl     1.03\n",
      "| epoch  51 |   212/  269 batches | lr 0.001784 |  3.55 ms | loss 0.02187 | ppl     1.02\n",
      "| epoch  51 |   265/  269 batches | lr 0.001784 |  3.50 ms | loss 0.01704 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time:  0.97s | valid loss 0.14175 | valid ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |    53/  269 batches | lr 0.001749 |  3.53 ms | loss 0.02869 | ppl     1.03\n",
      "| epoch  52 |   106/  269 batches | lr 0.001749 |  3.49 ms | loss 0.03529 | ppl     1.04\n",
      "| epoch  52 |   159/  269 batches | lr 0.001749 |  3.46 ms | loss 0.03420 | ppl     1.03\n",
      "| epoch  52 |   212/  269 batches | lr 0.001749 |  3.40 ms | loss 0.01806 | ppl     1.02\n",
      "| epoch  52 |   265/  269 batches | lr 0.001749 |  3.46 ms | loss 0.01837 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time:  0.97s | valid loss 0.12747 | valid ppl     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |    53/  269 batches | lr 0.001714 |  3.46 ms | loss 0.02945 | ppl     1.03\n",
      "| epoch  53 |   106/  269 batches | lr 0.001714 |  3.42 ms | loss 0.02096 | ppl     1.02\n",
      "| epoch  53 |   159/  269 batches | lr 0.001714 |  3.38 ms | loss 0.02517 | ppl     1.03\n",
      "| epoch  53 |   212/  269 batches | lr 0.001714 |  3.49 ms | loss 0.01556 | ppl     1.02\n",
      "| epoch  53 |   265/  269 batches | lr 0.001714 |  3.44 ms | loss 0.01737 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time:  0.96s | valid loss 0.11229 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |    53/  269 batches | lr 0.001679 |  3.49 ms | loss 0.02741 | ppl     1.03\n",
      "| epoch  54 |   106/  269 batches | lr 0.001679 |  3.46 ms | loss 0.01917 | ppl     1.02\n",
      "| epoch  54 |   159/  269 batches | lr 0.001679 |  3.47 ms | loss 0.02221 | ppl     1.02\n",
      "| epoch  54 |   212/  269 batches | lr 0.001679 |  3.48 ms | loss 0.01733 | ppl     1.02\n",
      "| epoch  54 |   265/  269 batches | lr 0.001679 |  3.46 ms | loss 0.01924 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time:  0.97s | valid loss 0.05777 | valid ppl     1.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |    53/  269 batches | lr 0.001646 |  3.49 ms | loss 0.02426 | ppl     1.02\n",
      "| epoch  55 |   106/  269 batches | lr 0.001646 |  3.44 ms | loss 0.01392 | ppl     1.01\n",
      "| epoch  55 |   159/  269 batches | lr 0.001646 |  3.46 ms | loss 0.02142 | ppl     1.02\n",
      "| epoch  55 |   212/  269 batches | lr 0.001646 |  3.38 ms | loss 0.01916 | ppl     1.02\n",
      "| epoch  55 |   265/  269 batches | lr 0.001646 |  3.44 ms | loss 0.01974 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time:  0.96s | valid loss 0.02855 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |    53/  269 batches | lr 0.001613 |  3.70 ms | loss 0.02286 | ppl     1.02\n",
      "| epoch  56 |   106/  269 batches | lr 0.001613 |  3.46 ms | loss 0.01362 | ppl     1.01\n",
      "| epoch  56 |   159/  269 batches | lr 0.001613 |  3.47 ms | loss 0.01781 | ppl     1.02\n",
      "| epoch  56 |   212/  269 batches | lr 0.001613 |  3.40 ms | loss 0.01905 | ppl     1.02\n",
      "| epoch  56 |   265/  269 batches | lr 0.001613 |  3.51 ms | loss 0.01869 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time:  0.98s | valid loss 0.02340 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |    53/  269 batches | lr 0.001581 |  3.54 ms | loss 0.02018 | ppl     1.02\n",
      "| epoch  57 |   106/  269 batches | lr 0.001581 |  3.51 ms | loss 0.01349 | ppl     1.01\n",
      "| epoch  57 |   159/  269 batches | lr 0.001581 |  3.46 ms | loss 0.01647 | ppl     1.02\n",
      "| epoch  57 |   212/  269 batches | lr 0.001581 |  3.42 ms | loss 0.01653 | ppl     1.02\n",
      "| epoch  57 |   265/  269 batches | lr 0.001581 |  3.40 ms | loss 0.01677 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time:  0.97s | valid loss 0.02011 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |    53/  269 batches | lr 0.001549 |  3.49 ms | loss 0.01834 | ppl     1.02\n",
      "| epoch  58 |   106/  269 batches | lr 0.001549 |  3.47 ms | loss 0.01354 | ppl     1.01\n",
      "| epoch  58 |   159/  269 batches | lr 0.001549 |  3.44 ms | loss 0.01503 | ppl     1.02\n",
      "| epoch  58 |   212/  269 batches | lr 0.001549 |  3.42 ms | loss 0.01718 | ppl     1.02\n",
      "| epoch  58 |   265/  269 batches | lr 0.001549 |  3.45 ms | loss 0.01659 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time:  0.96s | valid loss 0.01865 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |    53/  269 batches | lr 0.001518 |  3.51 ms | loss 0.01719 | ppl     1.02\n",
      "| epoch  59 |   106/  269 batches | lr 0.001518 |  3.42 ms | loss 0.01247 | ppl     1.01\n",
      "| epoch  59 |   159/  269 batches | lr 0.001518 |  3.42 ms | loss 0.01295 | ppl     1.01\n",
      "| epoch  59 |   212/  269 batches | lr 0.001518 |  3.44 ms | loss 0.01486 | ppl     1.01\n",
      "| epoch  59 |   265/  269 batches | lr 0.001518 |  3.49 ms | loss 0.01628 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time:  0.97s | valid loss 0.01932 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |    53/  269 batches | lr 0.001488 |  3.51 ms | loss 0.01685 | ppl     1.02\n",
      "| epoch  60 |   106/  269 batches | lr 0.001488 |  3.38 ms | loss 0.01193 | ppl     1.01\n",
      "| epoch  60 |   159/  269 batches | lr 0.001488 |  3.42 ms | loss 0.01309 | ppl     1.01\n",
      "| epoch  60 |   212/  269 batches | lr 0.001488 |  3.46 ms | loss 0.01378 | ppl     1.01\n",
      "| epoch  60 |   265/  269 batches | lr 0.001488 |  3.42 ms | loss 0.01546 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time:  2.25s | valid loss 0.02092 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |    53/  269 batches | lr 0.001458 |  3.51 ms | loss 0.01624 | ppl     1.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  61 |   106/  269 batches | lr 0.001458 |  3.44 ms | loss 0.01145 | ppl     1.01\n",
      "| epoch  61 |   159/  269 batches | lr 0.001458 |  3.46 ms | loss 0.01305 | ppl     1.01\n",
      "| epoch  61 |   212/  269 batches | lr 0.001458 |  3.46 ms | loss 0.01408 | ppl     1.01\n",
      "| epoch  61 |   265/  269 batches | lr 0.001458 |  3.51 ms | loss 0.01461 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time:  0.97s | valid loss 0.02194 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |    53/  269 batches | lr 0.001429 |  3.51 ms | loss 0.01597 | ppl     1.02\n",
      "| epoch  62 |   106/  269 batches | lr 0.001429 |  3.47 ms | loss 0.01145 | ppl     1.01\n",
      "| epoch  62 |   159/  269 batches | lr 0.001429 |  3.47 ms | loss 0.01224 | ppl     1.01\n",
      "| epoch  62 |   212/  269 batches | lr 0.001429 |  3.44 ms | loss 0.01446 | ppl     1.01\n",
      "| epoch  62 |   265/  269 batches | lr 0.001429 |  3.40 ms | loss 0.01331 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time:  0.97s | valid loss 0.02995 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |    53/  269 batches | lr 0.001400 |  3.57 ms | loss 0.01484 | ppl     1.01\n",
      "| epoch  63 |   106/  269 batches | lr 0.001400 |  3.41 ms | loss 0.01091 | ppl     1.01\n",
      "| epoch  63 |   159/  269 batches | lr 0.001400 |  3.42 ms | loss 0.01181 | ppl     1.01\n",
      "| epoch  63 |   212/  269 batches | lr 0.001400 |  3.42 ms | loss 0.01373 | ppl     1.01\n",
      "| epoch  63 |   265/  269 batches | lr 0.001400 |  3.42 ms | loss 0.01151 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time:  0.96s | valid loss 0.03012 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |    53/  269 batches | lr 0.001372 |  3.51 ms | loss 0.01370 | ppl     1.01\n",
      "| epoch  64 |   106/  269 batches | lr 0.001372 |  3.40 ms | loss 0.01069 | ppl     1.01\n",
      "| epoch  64 |   159/  269 batches | lr 0.001372 |  3.46 ms | loss 0.01275 | ppl     1.01\n",
      "| epoch  64 |   212/  269 batches | lr 0.001372 |  3.42 ms | loss 0.01243 | ppl     1.01\n",
      "| epoch  64 |   265/  269 batches | lr 0.001372 |  3.49 ms | loss 0.01006 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time:  0.97s | valid loss 0.03327 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |    53/  269 batches | lr 0.001345 |  3.47 ms | loss 0.01515 | ppl     1.02\n",
      "| epoch  65 |   106/  269 batches | lr 0.001345 |  3.44 ms | loss 0.00983 | ppl     1.01\n",
      "| epoch  65 |   159/  269 batches | lr 0.001345 |  3.46 ms | loss 0.01216 | ppl     1.01\n",
      "| epoch  65 |   212/  269 batches | lr 0.001345 |  3.38 ms | loss 0.01169 | ppl     1.01\n",
      "| epoch  65 |   265/  269 batches | lr 0.001345 |  3.44 ms | loss 0.01000 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time:  0.96s | valid loss 0.03185 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |    53/  269 batches | lr 0.001318 |  3.53 ms | loss 0.01663 | ppl     1.02\n",
      "| epoch  66 |   106/  269 batches | lr 0.001318 |  3.46 ms | loss 0.00992 | ppl     1.01\n",
      "| epoch  66 |   159/  269 batches | lr 0.001318 |  3.41 ms | loss 0.01260 | ppl     1.01\n",
      "| epoch  66 |   212/  269 batches | lr 0.001318 |  3.44 ms | loss 0.01180 | ppl     1.01\n",
      "| epoch  66 |   265/  269 batches | lr 0.001318 |  3.42 ms | loss 0.01055 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time:  0.96s | valid loss 0.03242 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |    53/  269 batches | lr 0.001292 |  3.47 ms | loss 0.01526 | ppl     1.02\n",
      "| epoch  67 |   106/  269 batches | lr 0.001292 |  3.47 ms | loss 0.00932 | ppl     1.01\n",
      "| epoch  67 |   159/  269 batches | lr 0.001292 |  3.44 ms | loss 0.01304 | ppl     1.01\n",
      "| epoch  67 |   212/  269 batches | lr 0.001292 |  3.63 ms | loss 0.01244 | ppl     1.01\n",
      "| epoch  67 |   265/  269 batches | lr 0.001292 |  3.49 ms | loss 0.00970 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time:  0.98s | valid loss 0.03749 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |    53/  269 batches | lr 0.001266 |  3.46 ms | loss 0.01470 | ppl     1.01\n",
      "| epoch  68 |   106/  269 batches | lr 0.001266 |  3.58 ms | loss 0.01042 | ppl     1.01\n",
      "| epoch  68 |   159/  269 batches | lr 0.001266 |  3.65 ms | loss 0.01382 | ppl     1.01\n",
      "| epoch  68 |   212/  269 batches | lr 0.001266 |  3.51 ms | loss 0.01478 | ppl     1.01\n",
      "| epoch  68 |   265/  269 batches | lr 0.001266 |  3.57 ms | loss 0.01079 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time:  0.99s | valid loss 0.04002 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |    53/  269 batches | lr 0.001240 |  3.51 ms | loss 0.01513 | ppl     1.02\n",
      "| epoch  69 |   106/  269 batches | lr 0.001240 |  3.38 ms | loss 0.01067 | ppl     1.01\n",
      "| epoch  69 |   159/  269 batches | lr 0.001240 |  3.64 ms | loss 0.01436 | ppl     1.01\n",
      "| epoch  69 |   212/  269 batches | lr 0.001240 |  3.43 ms | loss 0.01381 | ppl     1.01\n",
      "| epoch  69 |   265/  269 batches | lr 0.001240 |  3.44 ms | loss 0.01244 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time:  0.97s | valid loss 0.03420 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |    53/  269 batches | lr 0.001216 |  3.57 ms | loss 0.01539 | ppl     1.02\n",
      "| epoch  70 |   106/  269 batches | lr 0.001216 |  3.53 ms | loss 0.01025 | ppl     1.01\n",
      "| epoch  70 |   159/  269 batches | lr 0.001216 |  3.49 ms | loss 0.01401 | ppl     1.01\n",
      "| epoch  70 |   212/  269 batches | lr 0.001216 |  3.42 ms | loss 0.01526 | ppl     1.02\n",
      "| epoch  70 |   265/  269 batches | lr 0.001216 |  3.40 ms | loss 0.01561 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time:  2.22s | valid loss 0.02477 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  71 |    53/  269 batches | lr 0.001191 |  3.51 ms | loss 0.01529 | ppl     1.02\n",
      "| epoch  71 |   106/  269 batches | lr 0.001191 |  3.46 ms | loss 0.00988 | ppl     1.01\n",
      "| epoch  71 |   159/  269 batches | lr 0.001191 |  3.49 ms | loss 0.01267 | ppl     1.01\n",
      "| epoch  71 |   212/  269 batches | lr 0.001191 |  3.40 ms | loss 0.01511 | ppl     1.02\n",
      "| epoch  71 |   265/  269 batches | lr 0.001191 |  3.42 ms | loss 0.01489 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time:  0.97s | valid loss 0.01527 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |    53/  269 batches | lr 0.001167 |  3.53 ms | loss 0.01416 | ppl     1.01\n",
      "| epoch  72 |   106/  269 batches | lr 0.001167 |  3.55 ms | loss 0.01057 | ppl     1.01\n",
      "| epoch  72 |   159/  269 batches | lr 0.001167 |  3.64 ms | loss 0.01171 | ppl     1.01\n",
      "| epoch  72 |   212/  269 batches | lr 0.001167 |  3.57 ms | loss 0.01486 | ppl     1.01\n",
      "| epoch  72 |   265/  269 batches | lr 0.001167 |  3.57 ms | loss 0.01499 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time:  1.00s | valid loss 0.01270 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |    53/  269 batches | lr 0.001144 |  3.73 ms | loss 0.01283 | ppl     1.01\n",
      "| epoch  73 |   106/  269 batches | lr 0.001144 |  3.63 ms | loss 0.01062 | ppl     1.01\n",
      "| epoch  73 |   159/  269 batches | lr 0.001144 |  3.68 ms | loss 0.01130 | ppl     1.01\n",
      "| epoch  73 |   212/  269 batches | lr 0.001144 |  3.44 ms | loss 0.01442 | ppl     1.01\n",
      "| epoch  73 |   265/  269 batches | lr 0.001144 |  3.46 ms | loss 0.01403 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time:  1.00s | valid loss 0.01221 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |    53/  269 batches | lr 0.001121 |  3.60 ms | loss 0.01207 | ppl     1.01\n",
      "| epoch  74 |   106/  269 batches | lr 0.001121 |  3.49 ms | loss 0.01019 | ppl     1.01\n",
      "| epoch  74 |   159/  269 batches | lr 0.001121 |  3.42 ms | loss 0.01057 | ppl     1.01\n",
      "| epoch  74 |   212/  269 batches | lr 0.001121 |  3.42 ms | loss 0.01347 | ppl     1.01\n",
      "| epoch  74 |   265/  269 batches | lr 0.001121 |  3.42 ms | loss 0.01286 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time:  0.97s | valid loss 0.01289 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |    53/  269 batches | lr 0.001099 |  3.53 ms | loss 0.01183 | ppl     1.01\n",
      "| epoch  75 |   106/  269 batches | lr 0.001099 |  3.46 ms | loss 0.00959 | ppl     1.01\n",
      "| epoch  75 |   159/  269 batches | lr 0.001099 |  3.44 ms | loss 0.01067 | ppl     1.01\n",
      "| epoch  75 |   212/  269 batches | lr 0.001099 |  3.40 ms | loss 0.01302 | ppl     1.01\n",
      "| epoch  75 |   265/  269 batches | lr 0.001099 |  3.51 ms | loss 0.01229 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time:  0.97s | valid loss 0.01370 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |    53/  269 batches | lr 0.001077 |  3.49 ms | loss 0.01147 | ppl     1.01\n",
      "| epoch  76 |   106/  269 batches | lr 0.001077 |  3.46 ms | loss 0.00968 | ppl     1.01\n",
      "| epoch  76 |   159/  269 batches | lr 0.001077 |  3.42 ms | loss 0.01049 | ppl     1.01\n",
      "| epoch  76 |   212/  269 batches | lr 0.001077 |  3.42 ms | loss 0.01267 | ppl     1.01\n",
      "| epoch  76 |   265/  269 batches | lr 0.001077 |  3.46 ms | loss 0.01212 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time:  0.96s | valid loss 0.01424 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |    53/  269 batches | lr 0.001055 |  3.63 ms | loss 0.01221 | ppl     1.01\n",
      "| epoch  77 |   106/  269 batches | lr 0.001055 |  3.65 ms | loss 0.00942 | ppl     1.01\n",
      "| epoch  77 |   159/  269 batches | lr 0.001055 |  3.68 ms | loss 0.01038 | ppl     1.01\n",
      "| epoch  77 |   212/  269 batches | lr 0.001055 |  3.60 ms | loss 0.01196 | ppl     1.01\n",
      "| epoch  77 |   265/  269 batches | lr 0.001055 |  3.47 ms | loss 0.01154 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time:  1.01s | valid loss 0.01528 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |    53/  269 batches | lr 0.001034 |  3.46 ms | loss 0.01175 | ppl     1.01\n",
      "| epoch  78 |   106/  269 batches | lr 0.001034 |  3.42 ms | loss 0.00935 | ppl     1.01\n",
      "| epoch  78 |   159/  269 batches | lr 0.001034 |  3.46 ms | loss 0.00991 | ppl     1.01\n",
      "| epoch  78 |   212/  269 batches | lr 0.001034 |  3.48 ms | loss 0.01201 | ppl     1.01\n",
      "| epoch  78 |   265/  269 batches | lr 0.001034 |  3.69 ms | loss 0.01133 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time:  0.98s | valid loss 0.01744 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |    53/  269 batches | lr 0.001014 |  3.74 ms | loss 0.01112 | ppl     1.01\n",
      "| epoch  79 |   106/  269 batches | lr 0.001014 |  3.64 ms | loss 0.00921 | ppl     1.01\n",
      "| epoch  79 |   159/  269 batches | lr 0.001014 |  3.72 ms | loss 0.01091 | ppl     1.01\n",
      "| epoch  79 |   212/  269 batches | lr 0.001014 |  3.67 ms | loss 0.01210 | ppl     1.01\n",
      "| epoch  79 |   265/  269 batches | lr 0.001014 |  3.54 ms | loss 0.01118 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time:  1.02s | valid loss 0.01867 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |    53/  269 batches | lr 0.000993 |  3.49 ms | loss 0.01187 | ppl     1.01\n",
      "| epoch  80 |   106/  269 batches | lr 0.000993 |  3.49 ms | loss 0.00935 | ppl     1.01\n",
      "| epoch  80 |   159/  269 batches | lr 0.000993 |  3.46 ms | loss 0.01089 | ppl     1.01\n",
      "| epoch  80 |   212/  269 batches | lr 0.000993 |  3.42 ms | loss 0.01192 | ppl     1.01\n",
      "| epoch  80 |   265/  269 batches | lr 0.000993 |  3.61 ms | loss 0.01178 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time:  2.25s | valid loss 0.01838 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |    53/  269 batches | lr 0.000973 |  3.46 ms | loss 0.01193 | ppl     1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  81 |   106/  269 batches | lr 0.000973 |  3.49 ms | loss 0.00892 | ppl     1.01\n",
      "| epoch  81 |   159/  269 batches | lr 0.000973 |  3.61 ms | loss 0.01063 | ppl     1.01\n",
      "| epoch  81 |   212/  269 batches | lr 0.000973 |  3.47 ms | loss 0.01248 | ppl     1.01\n",
      "| epoch  81 |   265/  269 batches | lr 0.000973 |  3.45 ms | loss 0.01194 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time:  0.98s | valid loss 0.01777 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |    53/  269 batches | lr 0.000954 |  3.49 ms | loss 0.01147 | ppl     1.01\n",
      "| epoch  82 |   106/  269 batches | lr 0.000954 |  3.44 ms | loss 0.00902 | ppl     1.01\n",
      "| epoch  82 |   159/  269 batches | lr 0.000954 |  3.38 ms | loss 0.01098 | ppl     1.01\n",
      "| epoch  82 |   212/  269 batches | lr 0.000954 |  3.40 ms | loss 0.01277 | ppl     1.01\n",
      "| epoch  82 |   265/  269 batches | lr 0.000954 |  3.46 ms | loss 0.01187 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time:  0.96s | valid loss 0.01552 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |    53/  269 batches | lr 0.000935 |  3.55 ms | loss 0.01108 | ppl     1.01\n",
      "| epoch  83 |   106/  269 batches | lr 0.000935 |  3.40 ms | loss 0.00933 | ppl     1.01\n",
      "| epoch  83 |   159/  269 batches | lr 0.000935 |  3.40 ms | loss 0.01068 | ppl     1.01\n",
      "| epoch  83 |   212/  269 batches | lr 0.000935 |  3.40 ms | loss 0.01282 | ppl     1.01\n",
      "| epoch  83 |   265/  269 batches | lr 0.000935 |  3.42 ms | loss 0.01271 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time:  0.96s | valid loss 0.01438 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |    53/  269 batches | lr 0.000916 |  3.73 ms | loss 0.01226 | ppl     1.01\n",
      "| epoch  84 |   106/  269 batches | lr 0.000916 |  3.49 ms | loss 0.00939 | ppl     1.01\n",
      "| epoch  84 |   159/  269 batches | lr 0.000916 |  3.44 ms | loss 0.01046 | ppl     1.01\n",
      "| epoch  84 |   212/  269 batches | lr 0.000916 |  3.47 ms | loss 0.01239 | ppl     1.01\n",
      "| epoch  84 |   265/  269 batches | lr 0.000916 |  3.40 ms | loss 0.01175 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time:  0.98s | valid loss 0.01249 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |    53/  269 batches | lr 0.000898 |  3.42 ms | loss 0.01138 | ppl     1.01\n",
      "| epoch  85 |   106/  269 batches | lr 0.000898 |  3.46 ms | loss 0.00910 | ppl     1.01\n",
      "| epoch  85 |   159/  269 batches | lr 0.000898 |  3.47 ms | loss 0.00993 | ppl     1.01\n",
      "| epoch  85 |   212/  269 batches | lr 0.000898 |  3.63 ms | loss 0.01158 | ppl     1.01\n",
      "| epoch  85 |   265/  269 batches | lr 0.000898 |  3.52 ms | loss 0.01142 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time:  0.98s | valid loss 0.01089 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |    53/  269 batches | lr 0.000880 |  3.76 ms | loss 0.01068 | ppl     1.01\n",
      "| epoch  86 |   106/  269 batches | lr 0.000880 |  3.55 ms | loss 0.00879 | ppl     1.01\n",
      "| epoch  86 |   159/  269 batches | lr 0.000880 |  3.50 ms | loss 0.00935 | ppl     1.01\n",
      "| epoch  86 |   212/  269 batches | lr 0.000880 |  3.55 ms | loss 0.01076 | ppl     1.01\n",
      "| epoch  86 |   265/  269 batches | lr 0.000880 |  4.02 ms | loss 0.01072 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time:  1.03s | valid loss 0.01007 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |    53/  269 batches | lr 0.000862 |  3.70 ms | loss 0.01055 | ppl     1.01\n",
      "| epoch  87 |   106/  269 batches | lr 0.000862 |  3.59 ms | loss 0.00905 | ppl     1.01\n",
      "| epoch  87 |   159/  269 batches | lr 0.000862 |  3.62 ms | loss 0.00989 | ppl     1.01\n",
      "| epoch  87 |   212/  269 batches | lr 0.000862 |  3.68 ms | loss 0.01036 | ppl     1.01\n",
      "| epoch  87 |   265/  269 batches | lr 0.000862 |  3.54 ms | loss 0.01034 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time:  1.01s | valid loss 0.01007 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |    53/  269 batches | lr 0.000845 |  3.76 ms | loss 0.01042 | ppl     1.01\n",
      "| epoch  88 |   106/  269 batches | lr 0.000845 |  3.72 ms | loss 0.00860 | ppl     1.01\n",
      "| epoch  88 |   159/  269 batches | lr 0.000845 |  3.71 ms | loss 0.00960 | ppl     1.01\n",
      "| epoch  88 |   212/  269 batches | lr 0.000845 |  3.52 ms | loss 0.01008 | ppl     1.01\n",
      "| epoch  88 |   265/  269 batches | lr 0.000845 |  3.51 ms | loss 0.01015 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time:  1.02s | valid loss 0.01020 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |    53/  269 batches | lr 0.000828 |  3.59 ms | loss 0.01073 | ppl     1.01\n",
      "| epoch  89 |   106/  269 batches | lr 0.000828 |  3.50 ms | loss 0.00832 | ppl     1.01\n",
      "| epoch  89 |   159/  269 batches | lr 0.000828 |  3.52 ms | loss 0.00983 | ppl     1.01\n",
      "| epoch  89 |   212/  269 batches | lr 0.000828 |  3.51 ms | loss 0.01043 | ppl     1.01\n",
      "| epoch  89 |   265/  269 batches | lr 0.000828 |  3.62 ms | loss 0.00966 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time:  0.99s | valid loss 0.01046 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |    53/  269 batches | lr 0.000812 |  3.75 ms | loss 0.01061 | ppl     1.01\n",
      "| epoch  90 |   106/  269 batches | lr 0.000812 |  3.62 ms | loss 0.00795 | ppl     1.01\n",
      "| epoch  90 |   159/  269 batches | lr 0.000812 |  3.64 ms | loss 0.00977 | ppl     1.01\n",
      "| epoch  90 |   212/  269 batches | lr 0.000812 |  3.47 ms | loss 0.01010 | ppl     1.01\n",
      "| epoch  90 |   265/  269 batches | lr 0.000812 |  3.47 ms | loss 0.00966 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time:  2.27s | valid loss 0.01092 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |    53/  269 batches | lr 0.000795 |  3.69 ms | loss 0.01075 | ppl     1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwears\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  91 |   106/  269 batches | lr 0.000795 |  3.59 ms | loss 0.00824 | ppl     1.01\n",
      "| epoch  91 |   159/  269 batches | lr 0.000795 |  3.97 ms | loss 0.00939 | ppl     1.01\n",
      "| epoch  91 |   212/  269 batches | lr 0.000795 |  3.77 ms | loss 0.00976 | ppl     1.01\n",
      "| epoch  91 |   265/  269 batches | lr 0.000795 |  3.66 ms | loss 0.01004 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time:  1.05s | valid loss 0.01061 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |    53/  269 batches | lr 0.000779 |  3.67 ms | loss 0.00996 | ppl     1.01\n",
      "| epoch  92 |   106/  269 batches | lr 0.000779 |  3.55 ms | loss 0.00801 | ppl     1.01\n",
      "| epoch  92 |   159/  269 batches | lr 0.000779 |  3.63 ms | loss 0.00925 | ppl     1.01\n",
      "| epoch  92 |   212/  269 batches | lr 0.000779 |  3.69 ms | loss 0.00997 | ppl     1.01\n",
      "| epoch  92 |   265/  269 batches | lr 0.000779 |  3.63 ms | loss 0.01032 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time:  1.02s | valid loss 0.01089 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |    53/  269 batches | lr 0.000764 |  4.19 ms | loss 0.01065 | ppl     1.01\n",
      "| epoch  93 |   106/  269 batches | lr 0.000764 |  3.59 ms | loss 0.00817 | ppl     1.01\n",
      "| epoch  93 |   159/  269 batches | lr 0.000764 |  3.65 ms | loss 0.00942 | ppl     1.01\n",
      "| epoch  93 |   212/  269 batches | lr 0.000764 |  3.59 ms | loss 0.00979 | ppl     1.01\n",
      "| epoch  93 |   265/  269 batches | lr 0.000764 |  3.60 ms | loss 0.00956 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time:  1.04s | valid loss 0.01206 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |    53/  269 batches | lr 0.000749 |  3.78 ms | loss 0.01090 | ppl     1.01\n",
      "| epoch  94 |   106/  269 batches | lr 0.000749 |  3.55 ms | loss 0.00781 | ppl     1.01\n",
      "| epoch  94 |   159/  269 batches | lr 0.000749 |  3.70 ms | loss 0.00928 | ppl     1.01\n",
      "| epoch  94 |   212/  269 batches | lr 0.000749 |  3.63 ms | loss 0.00973 | ppl     1.01\n",
      "| epoch  94 |   265/  269 batches | lr 0.000749 |  3.54 ms | loss 0.00960 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time:  1.02s | valid loss 0.01169 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |    53/  269 batches | lr 0.000734 |  3.70 ms | loss 0.01009 | ppl     1.01\n",
      "| epoch  95 |   106/  269 batches | lr 0.000734 |  3.69 ms | loss 0.00792 | ppl     1.01\n",
      "| epoch  95 |   159/  269 batches | lr 0.000734 |  3.43 ms | loss 0.00896 | ppl     1.01\n",
      "| epoch  95 |   212/  269 batches | lr 0.000734 |  3.51 ms | loss 0.00971 | ppl     1.01\n",
      "| epoch  95 |   265/  269 batches | lr 0.000734 |  3.61 ms | loss 0.00994 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time:  1.01s | valid loss 0.01137 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |    53/  269 batches | lr 0.000719 |  3.81 ms | loss 0.00953 | ppl     1.01\n",
      "| epoch  96 |   106/  269 batches | lr 0.000719 |  3.74 ms | loss 0.00794 | ppl     1.01\n",
      "| epoch  96 |   159/  269 batches | lr 0.000719 |  3.55 ms | loss 0.00903 | ppl     1.01\n",
      "| epoch  96 |   212/  269 batches | lr 0.000719 |  3.61 ms | loss 0.00975 | ppl     1.01\n",
      "| epoch  96 |   265/  269 batches | lr 0.000719 |  3.53 ms | loss 0.00982 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time:  1.02s | valid loss 0.01207 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |    53/  269 batches | lr 0.000705 |  3.61 ms | loss 0.01033 | ppl     1.01\n",
      "| epoch  97 |   106/  269 batches | lr 0.000705 |  3.57 ms | loss 0.00826 | ppl     1.01\n",
      "| epoch  97 |   159/  269 batches | lr 0.000705 |  3.63 ms | loss 0.00869 | ppl     1.01\n",
      "| epoch  97 |   212/  269 batches | lr 0.000705 |  3.70 ms | loss 0.00984 | ppl     1.01\n",
      "| epoch  97 |   265/  269 batches | lr 0.000705 |  3.69 ms | loss 0.00931 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time:  1.02s | valid loss 0.01130 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |    53/  269 batches | lr 0.000690 |  3.73 ms | loss 0.00962 | ppl     1.01\n",
      "| epoch  98 |   106/  269 batches | lr 0.000690 |  3.72 ms | loss 0.00828 | ppl     1.01\n",
      "| epoch  98 |   159/  269 batches | lr 0.000690 |  3.70 ms | loss 0.00895 | ppl     1.01\n",
      "| epoch  98 |   212/  269 batches | lr 0.000690 |  3.66 ms | loss 0.00933 | ppl     1.01\n",
      "| epoch  98 |   265/  269 batches | lr 0.000690 |  3.76 ms | loss 0.00998 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time:  1.04s | valid loss 0.00999 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |    53/  269 batches | lr 0.000677 |  3.81 ms | loss 0.00937 | ppl     1.01\n",
      "| epoch  99 |   106/  269 batches | lr 0.000677 |  3.67 ms | loss 0.00813 | ppl     1.01\n",
      "| epoch  99 |   159/  269 batches | lr 0.000677 |  4.04 ms | loss 0.00845 | ppl     1.01\n",
      "| epoch  99 |   212/  269 batches | lr 0.000677 |  3.65 ms | loss 0.00912 | ppl     1.01\n",
      "| epoch  99 |   265/  269 batches | lr 0.000677 |  3.45 ms | loss 0.00981 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time:  1.04s | valid loss 0.00921 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |    53/  269 batches | lr 0.000663 |  3.69 ms | loss 0.00867 | ppl     1.01\n",
      "| epoch 100 |   106/  269 batches | lr 0.000663 |  3.46 ms | loss 0.00846 | ppl     1.01\n",
      "| epoch 100 |   159/  269 batches | lr 0.000663 |  3.46 ms | loss 0.00851 | ppl     1.01\n",
      "| epoch 100 |   212/  269 batches | lr 0.000663 |  3.46 ms | loss 0.00907 | ppl     1.01\n",
      "| epoch 100 |   265/  269 batches | lr 0.000663 |  3.47 ms | loss 0.00898 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time:  2.24s | valid loss 0.01008 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data()\n",
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.005 \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    \n",
    "    if(epoch % 10 == 0):\n",
    "        val_loss = plot_and_loss(model, val_data,epoch)\n",
    "        predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
